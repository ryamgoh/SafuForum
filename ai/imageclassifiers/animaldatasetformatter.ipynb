{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6a2e12c",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "In place of an NSFW classifier, we will train a CNN classifier that is able to detect whether a cat exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3484320",
   "metadata": {},
   "outputs": [],
   "source": [
    "CAT_DIR = 'data/cat'\n",
    "NON_CAT_DIR = 'data/non-cat'\n",
    "OUTPUT_DIR = 'data/balanced_dataset_output'\n",
    "SAMPLE_SIZE = 2000  # Set your desired sample size per class\n",
    "SEED = 42 # for reproducibility\n",
    "TEST_RATIO = 0.2  # 20% for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f58bff98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import random\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_image_paths(directory):\n",
    "    \"\"\"Recursively find all image file paths in a directory using Pathlib.\"\"\"\n",
    "    directory = Path(directory)\n",
    "    extensions = {'.png', '.jpg', '.jpeg', '.gif', '.bmp'}\n",
    "    return [p for p in directory.rglob('*') if p.suffix.lower() in extensions]\n",
    "\n",
    "def curate_dataset(cat_dir, non_cat_dir, output_dir, target_size, test_ratio=0.2, seed=42, clear_output_folder=False):\n",
    "    random.seed(seed)\n",
    "    output_path = Path(output_dir)\n",
    "    \n",
    "    # 1. Clean and Setup Output Directory\n",
    "    if output_path.exists():\n",
    "        if clear_output_folder:\n",
    "            print(f\"Clearing existing output directory: {output_path}\")\n",
    "            shutil.rmtree(output_path)\n",
    "        else:\n",
    "            print(f\"Warning: Output directory {output_path} exists and clear_output_folder is False.\")\n",
    "            # Depending on use case, you might want to return or raise error here\n",
    "            \n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # 2. Collect Files\n",
    "    print(\"Scanning directories...\")\n",
    "    cat_files = get_image_paths(cat_dir)\n",
    "    print(f\"Found {len(cat_files)} cat images.\")\n",
    "    non_cat_files = get_image_paths(non_cat_dir)\n",
    "    print(f\"Found {len(non_cat_files)} non-cat images.\")\n",
    "    \n",
    "    # 3. Handle Sample Size logic\n",
    "    available_min = min(len(cat_files), len(non_cat_files))\n",
    "    if available_min < target_size:\n",
    "        print(f\"⚠️ Insufficient data for target size {target_size}. Adjusting to {available_min}.\")\n",
    "        target_size = available_min\n",
    "\n",
    "    # 4. Select and Shuffle Samples\n",
    "    selected_cats = random.sample(cat_files, target_size)\n",
    "    selected_non_cats = random.sample(non_cat_files, target_size)\n",
    "\n",
    "    # 5. Calculate Split Index\n",
    "    split_idx = int(target_size * (1 - test_ratio))\n",
    "    \n",
    "    data_splits = {\n",
    "        'training_data': [\n",
    "            (selected_cats[:split_idx], 'cat', 1),\n",
    "            (selected_non_cats[:split_idx], 'non-cat', 0)\n",
    "        ],\n",
    "        'test_data': [\n",
    "            (selected_cats[split_idx:], 'cat', 1),\n",
    "            (selected_non_cats[split_idx:], 'non-cat', 0)\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # 6. Process Splits\n",
    "    for split_name, categories in data_splits.items():\n",
    "        # output/training_data\n",
    "        split_dir = output_path / split_name\n",
    "        split_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        csv_data = [] \n",
    "        \n",
    "        print(f\"\\nProcessing {split_name}...\")\n",
    "        \n",
    "        for files, class_name, class_id in categories:\n",
    "            \n",
    "            for file_path in tqdm(files, desc=f\"  Copying {class_name}\"):\n",
    "                # Create unique filename: 1_imageName.jpg\n",
    "                new_filename = f\"{class_id}_{file_path.name}\"\n",
    "                \n",
    "                # Destination: output/training_data/1_imageName.jpg\n",
    "                dest_path = split_dir / new_filename\n",
    "                \n",
    "                shutil.copy2(file_path, dest_path)\n",
    "                \n",
    "                # CSV Path: just the filename, because CSV is in the same folder\n",
    "                csv_data.append({\n",
    "                    'img_path': new_filename, \n",
    "                    'label': class_id\n",
    "                })\n",
    "\n",
    "        # Save the CSV\n",
    "        df = pd.DataFrame(csv_data)\n",
    "        df.to_csv(split_dir / 'annotations.csv', index=False)\n",
    "\n",
    "    print(f\"\\nSUCCESS! Dataset curated at: {output_path}\")\n",
    "    print(f\"Structure:\")\n",
    "    print(f\"  ├── training_data/ (images + annotations.csv)\")\n",
    "    print(f\"  └── test_data/     (images + annotations.csv)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d58c59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing output directory: data/balanced_dataset_output\n",
      "Scanning directories...\n",
      "Found 1668 cat images.\n",
      "Found 24511 non-cat images.\n",
      "⚠️ Insufficient data for target size 2000. Adjusting to 1668.\n",
      "\n",
      "Processing training_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Copying cat: 100%|██████████| 1334/1334 [00:00<00:00, 4643.06it/s]\n",
      "  Copying non-cat: 100%|██████████| 1334/1334 [00:00<00:00, 6323.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing test_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Copying cat: 100%|██████████| 334/334 [00:00<00:00, 4982.12it/s]\n",
      "  Copying non-cat: 100%|██████████| 334/334 [00:00<00:00, 8955.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SUCCESS! Dataset curated at: data/balanced_dataset_output\n",
      "Structure:\n",
      "  ├── training_data/ (images + annotations.csv)\n",
      "  └── test_data/     (images + annotations.csv)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 1. Run the curation logic\n",
    "curate_dataset(\n",
    "    cat_dir=CAT_DIR,\n",
    "    non_cat_dir=NON_CAT_DIR,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    target_size=SAMPLE_SIZE,\n",
    "    seed=SEED,\n",
    "    test_ratio=TEST_RATIO,\n",
    "    clear_output_folder=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

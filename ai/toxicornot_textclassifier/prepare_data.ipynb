{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcda28ce",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "## OpinRank Dataset\n",
    "[Download the dataset here](https://github.com/kavgan/OpinRank/blob/master/OpinRankDatasetWithJudgments.zip)\n",
    "\n",
    "## Kaggle Toxic Comment Classification Challenge\n",
    "[Download the dataset here](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data)\n",
    "```bash\n",
    "# Unzip all files in the current directory\n",
    "# The cwd should contain only the zip files to be extracted\n",
    "for file in *.zip; do unzip \"$file\"; done\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec08153f",
   "metadata": {},
   "source": [
    "# Brown Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57a8b5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/ryam/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/brown.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('brown')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b9ba8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1161192 entries, 0 to 1161191\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count    Dtype \n",
      "---  ------  --------------    ----- \n",
      " 0   text    1161192 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 8.9+ MB\n"
     ]
    }
   ],
   "source": [
    "# We want to make a big dataset btwn brown and kaggle toxic classifier\n",
    "from nltk.corpus import brown\n",
    "import pandas as pd\n",
    "\n",
    "# Get Brown corpus as DataFrame\n",
    "brown_data = brown.words()\n",
    "df_brown = pd.DataFrame(brown_data, columns=['text'])\n",
    "df_brown.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71e4bc26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11896096 entries, 0 to 11896095\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Dtype \n",
      "---  ------  ----- \n",
      " 0   text    object\n",
      "dtypes: object(1)\n",
      "memory usage: 90.8+ MB\n"
     ]
    }
   ],
   "source": [
    "# Read only the 'comment_text' column to save memory\n",
    "kaggle_data = pd.read_csv('data/train.csv', usecols=['comment_text'])\n",
    "\n",
    "# Split all Kaggle comments into words and make DataFrame\n",
    "kaggle_words = \" \".join(kaggle_data['comment_text'].astype(str)).split()\n",
    "df_kaggle = pd.DataFrame(kaggle_words, columns=['text'])\n",
    "\n",
    "# Append Kaggle words to Brown DataFrame\n",
    "df_all = pd.concat([df_brown, df_kaggle], ignore_index=True)\n",
    "\n",
    "# Show first few rows\n",
    "df_all.head()\n",
    "df_all.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1330f25f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ryam/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5995055 entries, 0 to 5995054\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Dtype \n",
      "---  ------  ----- \n",
      " 0   text    object\n",
      "dtypes: object(1)\n",
      "memory usage: 45.7+ MB\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Function to clean a single token\n",
    "def clean_token(token):\n",
    "    # Remove non-alphabetic characters and convert to lowercase\n",
    "    token = re.sub(r'[^a-zA-Z]', '', token)\n",
    "    return token.lower()\n",
    "\n",
    "# Apply cleaning to all tokens\n",
    "df_all['text'] = df_all['text'].apply(clean_token)\n",
    "\n",
    "# Remove empty strings\n",
    "df_all = df_all[df_all['text'] != '']\n",
    "\n",
    "# Optionally, remove stopwords\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords', download_dir='./data')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df_all = df_all[~df_all['text'].isin(stop_words)]\n",
    "\n",
    "df_all = df_all.reset_index(drop=True)\n",
    "df_all.head()\n",
    "df_all.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

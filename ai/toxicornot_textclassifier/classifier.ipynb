{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f212da03",
   "metadata": {},
   "source": [
    "# Make Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0341c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 8 columns):\n",
      " #   Column         Non-Null Count   Dtype \n",
      "---  ------         --------------   ----- \n",
      " 0   id             159571 non-null  object\n",
      " 1   comment_text   159571 non-null  object\n",
      " 2   toxic          159571 non-null  int64 \n",
      " 3   severe_toxic   159571 non-null  int64 \n",
      " 4   obscene        159571 non-null  int64 \n",
      " 5   threat         159571 non-null  int64 \n",
      " 6   insult         159571 non-null  int64 \n",
      " 7   identity_hate  159571 non-null  int64 \n",
      "dtypes: int64(6), object(2)\n",
      "memory usage: 9.7+ MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset from train.csv\n",
    "df_train = pd.read_csv('data/train.csv')\n",
    "\n",
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa6dd0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toxicity columns\n",
    "toxicity_columns = [\n",
    "    'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4c5edf",
   "metadata": {},
   "source": [
    "# 1. Removal of Stop Words\n",
    "\n",
    "Stop words are the words which are commonly used and removed from the sentence as pre-step in different Natural Language Processing (NLP) tasks. Example of stop words are: ‘a’, ‘an’, ‘the’, ‘this’, ‘not’ etc. Every tool uses a bit different set of stop words list that it removes but this technique is avoided in cases where phrase structure matters like in this case of Sentiment Analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccec0ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "\n",
    "df_train['text_stopwords_removed'] = df_train['comment_text'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5660a21",
   "metadata": {},
   "source": [
    "# 2. Tokenization\n",
    "\n",
    "Tokenization is the process in which the sentence/text is split into array of words called tokens. This helps to do transformations on each words separately and this is also required to transform words to numbers. There are different ways of performing tokenization. I have explained these ways in my previous post under Tokenization section, so if you are interested you can check it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be8c690f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenized_text_without_stopwords</th>\n",
       "      <th>text_stopwords_removed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[explanation, why, edits, username, hardcore, ...</td>\n",
       "      <td>Explanation Why edits username Hardcore Metall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[aww, he, matches, background, colour, seeming...</td>\n",
       "      <td>D'aww! He matches background colour I'm seemin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[hey, man, trying, edit, war, it, guy, constan...</td>\n",
       "      <td>Hey man, I'm trying edit war. It's guy constan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[more, can, real, suggestions, improvement, wo...</td>\n",
       "      <td>\" More I can't real suggestions improvement - ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[you, sir, hero, any, chance, remember, page, ...</td>\n",
       "      <td>You, sir, hero. Any chance remember page that'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[congratulations, well, use, tools, well, talk]</td>\n",
       "      <td>\" Congratulations well, use tools well. · talk \"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[cocksucker, before, you, piss, around, on, my...</td>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[your, vandalism, matt, shirvington, article, ...</td>\n",
       "      <td>Your vandalism Matt Shirvington article revert...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[sorry, word, nonsense, offensive, you, anyway...</td>\n",
       "      <td>Sorry word 'nonsense' offensive you. Anyway, I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[alignment, subject, contrary, dulithgow]</td>\n",
       "      <td>alignment subject contrary DuLithgow</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    tokenized_text_without_stopwords  \\\n",
       "0  [explanation, why, edits, username, hardcore, ...   \n",
       "1  [aww, he, matches, background, colour, seeming...   \n",
       "2  [hey, man, trying, edit, war, it, guy, constan...   \n",
       "3  [more, can, real, suggestions, improvement, wo...   \n",
       "4  [you, sir, hero, any, chance, remember, page, ...   \n",
       "5    [congratulations, well, use, tools, well, talk]   \n",
       "6  [cocksucker, before, you, piss, around, on, my...   \n",
       "7  [your, vandalism, matt, shirvington, article, ...   \n",
       "8  [sorry, word, nonsense, offensive, you, anyway...   \n",
       "9          [alignment, subject, contrary, dulithgow]   \n",
       "\n",
       "                              text_stopwords_removed  \n",
       "0  Explanation Why edits username Hardcore Metall...  \n",
       "1  D'aww! He matches background colour I'm seemin...  \n",
       "2  Hey man, I'm trying edit war. It's guy constan...  \n",
       "3  \" More I can't real suggestions improvement - ...  \n",
       "4  You, sir, hero. Any chance remember page that'...  \n",
       "5   \" Congratulations well, use tools well. · talk \"  \n",
       "6       COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK  \n",
       "7  Your vandalism Matt Shirvington article revert...  \n",
       "8  Sorry word 'nonsense' offensive you. Anyway, I...  \n",
       "9               alignment subject contrary DuLithgow  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "# Tokenize the text column to get the new column 'tokenized_text'\n",
    "df_train['tokenized_text_without_stopwords'] = [simple_preprocess(line, deacc=True) for line in df_train['text_stopwords_removed']]\n",
    "\n",
    "# Create a is_toxic column: 1 if any label is 1, else 0\n",
    "df_train['result'] = df_train[toxicity_columns].max(axis=1)\n",
    "\n",
    "df_train[['tokenized_text_without_stopwords', 'text_stopwords_removed']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccabe039",
   "metadata": {},
   "source": [
    "# 3. Stemming\n",
    "\n",
    "Stemming process reduces the words to its’ root word. Unlike Lemmatization which uses grammar rules and dictionary for mapping words to root form, stemming simply removes suffixes/prefixes. Stemming is widely used in the application of SEOs, Web search results, and information retrieval since as long as the root matches in the text somewhere it helps to retrieve all the related documents in the search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3bf48f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stemmed_tokens</th>\n",
       "      <th>tokenized_text_without_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[explan, why, edit, usernam, hardcor, metallic...</td>\n",
       "      <td>[explanation, why, edits, username, hardcore, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[aww, he, match, background, colour, seemingli...</td>\n",
       "      <td>[aww, he, matches, background, colour, seeming...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[hei, man, try, edit, war, it, gui, constantli...</td>\n",
       "      <td>[hey, man, trying, edit, war, it, guy, constan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[more, can, real, suggest, improv, wonder, sec...</td>\n",
       "      <td>[more, can, real, suggestions, improvement, wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[you, sir, hero, ani, chanc, rememb, page, tha...</td>\n",
       "      <td>[you, sir, hero, any, chance, remember, page, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[congratul, well, us, tool, well, talk]</td>\n",
       "      <td>[congratulations, well, use, tools, well, talk]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[cocksuck, befor, you, piss, around, on, my, w...</td>\n",
       "      <td>[cocksucker, before, you, piss, around, on, my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[your, vandal, matt, shirvington, articl, reve...</td>\n",
       "      <td>[your, vandalism, matt, shirvington, article, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[sorri, word, nonsens, offens, you, anywai, in...</td>\n",
       "      <td>[sorry, word, nonsense, offensive, you, anyway...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[align, subject, contrari, dulithgow]</td>\n",
       "      <td>[alignment, subject, contrary, dulithgow]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      stemmed_tokens  \\\n",
       "0  [explan, why, edit, usernam, hardcor, metallic...   \n",
       "1  [aww, he, match, background, colour, seemingli...   \n",
       "2  [hei, man, try, edit, war, it, gui, constantli...   \n",
       "3  [more, can, real, suggest, improv, wonder, sec...   \n",
       "4  [you, sir, hero, ani, chanc, rememb, page, tha...   \n",
       "5            [congratul, well, us, tool, well, talk]   \n",
       "6  [cocksuck, befor, you, piss, around, on, my, w...   \n",
       "7  [your, vandal, matt, shirvington, articl, reve...   \n",
       "8  [sorri, word, nonsens, offens, you, anywai, in...   \n",
       "9              [align, subject, contrari, dulithgow]   \n",
       "\n",
       "                    tokenized_text_without_stopwords  \n",
       "0  [explanation, why, edits, username, hardcore, ...  \n",
       "1  [aww, he, matches, background, colour, seeming...  \n",
       "2  [hey, man, trying, edit, war, it, guy, constan...  \n",
       "3  [more, can, real, suggestions, improvement, wo...  \n",
       "4  [you, sir, hero, any, chance, remember, page, ...  \n",
       "5    [congratulations, well, use, tools, well, talk]  \n",
       "6  [cocksucker, before, you, piss, around, on, my...  \n",
       "7  [your, vandalism, matt, shirvington, article, ...  \n",
       "8  [sorry, word, nonsense, offensive, you, anyway...  \n",
       "9          [alignment, subject, contrary, dulithgow]  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.parsing.porter import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "df_train['stemmed_tokens'] = [[porter_stemmer.stem(word) for word in tokens] for tokens in df_train['tokenized_text_without_stopwords'] ]\n",
    "\n",
    "df_train[['stemmed_tokens', 'tokenized_text_without_stopwords']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bae0a5",
   "metadata": {},
   "source": [
    "# Split Dataset into Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f762999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value counts for Train sentiments before resampling\n",
      "result\n",
      "0    100315\n",
      "1     11384\n",
      "Name: count, dtype: int64\n",
      "Value counts for Train Resampled sentiments\n",
      "result\n",
      "0    100315\n",
      "1    100315\n",
      "Name: count, dtype: int64\n",
      "Value counts for Test sentiments\n",
      "result\n",
      "0    43031\n",
      "1     4841\n",
      "Name: count, dtype: int64\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.series.Series'>\n",
      "                 id                                       comment_text  toxic  \\\n",
      "0  52772c567777757c  and being a female you are acting so cruel whi...      0   \n",
      "1  fcf34664ac5087d2  ==============================================...      0   \n",
      "2  828d5c69c2f283a0  \"\\n\\nNoitall. Like I have said all I am trying...      0   \n",
      "3  6161150e0239aaff  \"\\nOh, look how sweet he is. And what happened...      0   \n",
      "4  176fc566031ab49a  \"\\n\\n Alleged Moon Landing \\n\\nUntil it's prov...      0   \n",
      "\n",
      "   severe_toxic  obscene  threat  insult  identity_hate  \\\n",
      "0             0        0       0       0              0   \n",
      "1             0        0       0       0              0   \n",
      "2             0        0       0       0              0   \n",
      "3             0        0       0       0              0   \n",
      "4             0        0       0       0              0   \n",
      "\n",
      "                              text_stopwords_removed  \\\n",
      "0                     female acting cruel surprising   \n",
      "1  ==============================================...   \n",
      "2  \" Noitall. Like I said I trying contribute. I ...   \n",
      "3  \" Oh, look sweet is. And happened comments cal...   \n",
      "4  \" Alleged Moon Landing Until it's proven evide...   \n",
      "\n",
      "                    tokenized_text_without_stopwords  \\\n",
      "0                [female, acting, cruel, surprising]   \n",
      "1  [no, preceding, piece, uniqueness, holocaust, ...   \n",
      "2  [noitall, like, said, trying, contribute, didn...   \n",
      "3  [oh, look, sweet, is, and, happened, comments,...   \n",
      "4  [alleged, moon, landing, until, it, proven, ev...   \n",
      "\n",
      "                                      stemmed_tokens  \n",
      "0                       [femal, act, cruel, surpris]  \n",
      "1  [no, preced, piec, uniqu, holocaust, term, int...  \n",
      "2  [noital, like, said, try, contribut, didn, sta...  \n",
      "3  [oh, look, sweet, is, and, happen, comment, ca...  \n",
      "4  [alleg, moon, land, until, it, proven, evid, s...  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "200570                       [cunt, cunt, cunt, cunt, cunt]\n",
       "200571    [what, dumbass, slimvirgin, which, make, reali...\n",
       "200572    [rape, kati, perri, peni, night, dildo, fun, s...\n",
       "200573                          [hi, wanna, suck, ur, cock]\n",
       "200574    [februari, utc, perhap, quit, remov, comment, ...\n",
       "200575    [and, sourc, suffici, reliabl, accus, amount, ...\n",
       "200576              [want, an, answer, you, son, of, bitch]\n",
       "200577                                                   []\n",
       "200578                         [onorem, is, still, faugott]\n",
       "200579    [reckon, stand, fred, phelp, never, stand, hom...\n",
       "200580                   [fuck, you, re, drunk, son, bitch]\n",
       "200581    [flai, aliv, fking, stalker, shall, enjoi, scr...\n",
       "200582    [you, gayreek, don, open, mouth, stink, sperm,...\n",
       "200583    [import, note, charl, vandal, troll, malcont, ...\n",
       "200584    [guyzero, bite, me, go, ahead, and, block, the...\n",
       "200585    [rick, the, dick, check, out, the, common, pag...\n",
       "200586    [anim, feel, free, eras, becaus, don, know, wi...\n",
       "200587                         [think, add, tag, rid, crap]\n",
       "200588    [got, fellow, loser, tell, you, think, fuck, b...\n",
       "200589    [hi, pompou, ass, know, think, you, re, kind, ...\n",
       "200590    [re, move, dragon, slayer, dragon, slayer, ser...\n",
       "200591    [defens, drkiernan, voic, reason, anna, anders...\n",
       "200592    [have, no, life, and, if, don, get, to, ever, ...\n",
       "200593                             [holi, shit, holi, shit]\n",
       "200594    [topic, agree, suck, think, it, crappi, presid...\n",
       "200595    [oh, diddum, hit, sore, spot, person, attack, ...\n",
       "200596    [thi, articl, horrend, thi, worst, outrag, uni...\n",
       "200597    [poor, wolfi, delud, you, ad, us, pd, articl, ...\n",
       "200598    [ping, pong, shut, up, thi, busi, bear, rollin...\n",
       "200599    [administr, elitist, bastard, don, care, sai, ...\n",
       "200600    [insan, you, delet, my, userpag, with, the, cl...\n",
       "200601                                   [ar, faggot, what]\n",
       "200602                             [time, fun, littl, dick]\n",
       "200603    [fundras, ass, set, asid, protyp, psychot, par...\n",
       "200604                                  [ask, your, mother]\n",
       "200605    [ha, gai, hahahahaha, damn, thei, told, straig...\n",
       "200606    [yeah, whatev, you, arrog, delud, buffoon, def...\n",
       "200607                     [pleas, swallow, peni, homosexu]\n",
       "200608    [mayb, look, harass, is, pleas, chang, reason,...\n",
       "200609          [hei, renata, suck, black, cock, new, york]\n",
       "200610                       [thank, award, gay, you, suck]\n",
       "200611    [ok, let, come, on, gui, piec, shit, jew, mean...\n",
       "200612    [stop, ly, edit, my, good, faith, remov, clear...\n",
       "200613    [lmao, that, basic, sai, wahhh, call, dumb, gu...\n",
       "200614    [web, trust, it, fault, gai, littl, faggot, va...\n",
       "200615    [sigh, so, went, ahead, remov, anywai, page, l...\n",
       "200616                      [you, useless, prick, die, aid]\n",
       "200617    [have, clear, yet, it, pretti, fuck, obviou, d...\n",
       "200618                                  [trout, slap, dick]\n",
       "200619    [your, user, page, well, strang, look, thi, no...\n",
       "200620                                 [fuck, inch, weiner]\n",
       "200621                [stop, im, try, prank, sister, dummi]\n",
       "200622    [hei, hei, fuck, you, disrupt, edit, all, you,...\n",
       "200623    [oh, and, kuru, if, like, map, explain, map, w...\n",
       "200624                                                   []\n",
       "200625    [you, undid, chang, articl, school, probabl, k...\n",
       "200626    [yo, go, imperson, me, justic, homi, chequ, th...\n",
       "200627                       [then, yunshi, can, cock, ass]\n",
       "200628    [stupid, piec, shit, pleas, time, read, contri...\n",
       "200629    [hei, idiot, repeatedli, undo, my, edit, go, a...\n",
       "Name: stemmed_tokens, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "# Train Test Split Function\n",
    "def split_train_test(df_train, test_size=0.3, shuffle_state=True):\n",
    "    # 1. Split first\n",
    "    X = df_train.drop(columns=['result'])\n",
    "    y = df_train['result']\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.3, random_state=15, shuffle=True)\n",
    "\n",
    "    # 2. Resample only the training set\n",
    "    ros = RandomOverSampler(random_state=42)\n",
    "    X_train_resampled, Y_train_resampled = ros.fit_resample(X_train, Y_train)\n",
    "    print(\"Value counts for Train sentiments before resampling\")\n",
    "    print(Y_train.value_counts())\n",
    "    print(\"Value counts for Train Resampled sentiments\")\n",
    "    print(Y_train_resampled.value_counts())\n",
    "    print(\"Value counts for Test sentiments\")\n",
    "    print(Y_test.value_counts())\n",
    "    print(type(X_train_resampled))\n",
    "    print(type(Y_train_resampled))\n",
    "    X_train_resampled = X_train_resampled.reset_index(drop=True)\n",
    "    X_test = X_test.reset_index(drop=True)\n",
    "    Y_train_resampled = Y_train_resampled.reset_index(drop=True)\n",
    "    Y_test = Y_test.reset_index(drop=True)\n",
    "    print(X_train_resampled.head())\n",
    "    return X_train_resampled, X_test, Y_train_resampled, Y_test \n",
    "\n",
    "# Call the train_test_split\n",
    "X_train, X_test, Y_train, Y_test = split_train_test(df_train)\n",
    "\n",
    "X_train['stemmed_tokens'].tail(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9228d4",
   "metadata": {},
   "source": [
    "# Train Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69f0152a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to train word2vec model: 29.058432817459106\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import time\n",
    "# Skip-gram model (sg = 1)\n",
    "size = 500\n",
    "window = 3\n",
    "min_count = 1\n",
    "workers = 8\n",
    "sg = 1\n",
    "\n",
    "OUTPUT_FOLDER = 'data/'\n",
    "\n",
    "word2vec_model_file = OUTPUT_FOLDER + 'word2vec_' + str(size) + '.model'\n",
    "start_time = time.time()\n",
    "stemmed_tokens = pd.Series(df_train['stemmed_tokens']).values\n",
    "# Train the Word2Vec Model\n",
    "w2v_model = Word2Vec(stemmed_tokens, min_count = min_count, vector_size = size, workers = workers, window = window, sg = sg)\n",
    "print(\"Time taken to train word2vec model: \" + str(time.time() - start_time))\n",
    "w2v_model.save(word2vec_model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd5ebf7",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d555ec9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of the word 'action':\n",
      "311\n",
      "Vector generated for the word 'action':\n",
      "[-7.02268556e-02  2.64004111e-01  3.49453501e-02  8.36413875e-02\n",
      " -1.04161307e-01 -3.36617976e-01  3.51412803e-01  3.34654212e-01\n",
      "  1.80916145e-01 -9.83745530e-02  2.29382336e-01 -1.83328748e-01\n",
      "  2.50296026e-01 -4.66472208e-02  6.54150844e-02 -4.82346527e-02\n",
      "  7.50428587e-02 -2.22390518e-01 -1.73915878e-01 -1.66600481e-01\n",
      "  4.00683843e-03 -2.67991781e-01 -1.91588089e-01 -2.29298130e-01\n",
      "  9.22734737e-02 -3.40974003e-01  2.24473774e-02  1.63289700e-02\n",
      " -2.97569901e-01  2.46230006e-01  1.34298891e-01  1.36645615e-01\n",
      " -2.29113802e-01  8.81111547e-02 -1.41383726e-02 -5.15145436e-02\n",
      "  1.14522144e-01 -3.69274765e-01 -8.27937052e-02 -1.35492876e-01\n",
      " -4.29192364e-01 -8.82483050e-02 -6.59475505e-01  1.59609213e-01\n",
      " -1.00657500e-01 -6.68454766e-02  5.95811792e-02 -1.47030249e-01\n",
      " -1.77758053e-01 -1.89619020e-01 -5.32316081e-02  7.62736946e-02\n",
      " -8.84717479e-02  7.46044703e-03 -1.23321831e-01  8.97745565e-02\n",
      " -5.47409318e-02 -3.37392837e-02 -1.68289468e-01  2.61102319e-01\n",
      "  1.14799812e-01 -2.39253536e-01 -3.01798321e-02  1.81501154e-02\n",
      " -1.20501079e-01 -4.61732931e-02 -7.69834295e-02  1.32316172e-01\n",
      "  3.41503412e-01 -1.42154977e-01 -2.09546402e-01 -4.31461073e-02\n",
      " -1.53810456e-01  1.69807419e-01  5.21091282e-01 -3.66636887e-02\n",
      " -3.08867365e-01  4.53755632e-02  1.67816713e-01  2.90769320e-02\n",
      " -4.63157892e-02  4.68868583e-01 -2.02645004e-01 -1.75059780e-01\n",
      " -2.31256425e-01  1.39190331e-01  1.96466208e-01 -6.40392900e-02\n",
      "  3.00939292e-01  3.14428687e-01  8.03646743e-02  4.16490622e-02\n",
      " -8.44695121e-02 -1.17197677e-01 -9.09147337e-02  2.44863972e-01\n",
      " -1.27165392e-01  3.53027850e-01  9.39102620e-02  9.02364729e-04\n",
      " -6.27358630e-02 -3.27560812e-01  8.99576545e-02 -2.15277568e-01\n",
      "  2.97550410e-01  1.26677677e-01  9.80479419e-02  7.37629384e-02\n",
      " -5.79334609e-02 -2.94725094e-02  6.73816539e-03 -6.71355724e-02\n",
      "  3.35509688e-01  2.97167987e-01  1.39961407e-01  3.15826423e-02\n",
      " -1.74989998e-02 -4.86280136e-02 -6.35166839e-02 -2.93630004e-01\n",
      "  4.55270298e-02 -9.27044600e-02  2.19621524e-01  9.59036946e-02\n",
      "  2.97395825e-01  9.43888575e-02 -1.81301326e-01  2.36828864e-01\n",
      " -8.52828994e-02  1.26487404e-01  9.96200666e-02  4.71115202e-01\n",
      " -9.39779431e-02 -1.56621411e-01  2.14008614e-01  2.39697710e-01\n",
      " -6.32026419e-02 -1.09106027e-01  6.43236344e-05 -4.40630168e-01\n",
      " -1.37475625e-01 -2.93127149e-01 -5.18288091e-02  9.91224349e-02\n",
      "  3.95811021e-01  2.52394170e-01  2.14972213e-01 -9.63117778e-02\n",
      " -3.05675454e-02  3.16451669e-01 -1.06775589e-01  2.18757503e-02\n",
      " -7.20925108e-02  6.76092207e-02 -5.10165811e-01  5.82743995e-02\n",
      "  1.29034549e-01  2.60765284e-01 -7.16335773e-02 -1.69817403e-01\n",
      "  4.45868773e-03  1.57556891e-01 -4.39560086e-01 -6.99004233e-02\n",
      " -2.28768289e-01  1.02443106e-01  2.35122055e-01  2.44674563e-01\n",
      "  4.27008837e-01  8.81672725e-02  4.09926772e-01  9.95365456e-02\n",
      "  2.94746310e-01  2.64070123e-01 -2.39468977e-01 -1.62947655e-01\n",
      " -1.96030974e-01 -6.73975497e-02 -1.87870100e-01  2.48709202e-01\n",
      " -4.11328524e-02 -3.77121978e-02  2.32642621e-01  1.26441985e-01\n",
      " -3.58094305e-01  1.53227195e-01 -2.58199722e-01  1.39235198e-01\n",
      "  1.40351921e-01 -9.84511599e-02  2.05932662e-01 -4.24248874e-01\n",
      "  1.97459772e-01 -8.81425887e-02  4.12839979e-01 -3.38524669e-01\n",
      " -3.00571740e-01 -3.14636469e-01  2.35492419e-02 -2.56350994e-01\n",
      "  2.74891585e-01 -1.02402113e-01  2.88972944e-01 -1.87158972e-01\n",
      "  6.64660521e-03  1.67012706e-01 -1.45146638e-01 -2.39542514e-01\n",
      " -4.77697819e-01  3.06851596e-01 -8.12993050e-02 -4.92384344e-01\n",
      " -1.45892903e-01 -4.83792126e-02 -1.21635810e-01 -5.73335469e-01\n",
      " -2.67628670e-01 -8.99571404e-02 -9.45709869e-02 -3.74238312e-01\n",
      "  4.43864703e-01  2.18465343e-01  3.48632067e-01  2.26491481e-01\n",
      "  1.72189921e-01 -2.96777859e-02  4.92843613e-02  3.05837005e-01\n",
      " -1.26826704e-01 -8.33168849e-02  1.10801913e-01 -2.93049991e-01\n",
      "  9.47631076e-02 -1.78298905e-01 -6.36999756e-02 -1.02830388e-01\n",
      " -2.63456386e-02 -1.29707679e-01  1.67881325e-01  3.45785677e-01\n",
      "  4.10560817e-01  2.40184218e-01 -4.08179760e-01  7.26153776e-02\n",
      "  1.24253094e-01 -1.60373077e-01 -7.05141723e-02 -2.70788252e-01\n",
      " -3.35989445e-02  3.05246621e-01  5.34791052e-01 -9.51930974e-03\n",
      " -2.10955162e-02 -1.00206129e-01  7.74432346e-02  9.70533192e-02\n",
      "  3.83704096e-01  4.27680433e-01 -2.26175021e-02 -6.44336194e-02\n",
      " -4.79380377e-02  1.97808713e-01 -1.60996065e-01  1.83901444e-01\n",
      " -1.46642048e-02 -1.98421150e-01 -2.90709466e-01  5.40622175e-01\n",
      " -5.84175214e-02 -1.73020229e-01 -1.33336157e-01 -1.21348985e-01\n",
      "  2.42963314e-01 -1.46165654e-01 -2.76110470e-01  4.18616012e-02\n",
      " -1.22190312e-01 -6.89683199e-01 -3.95703495e-01  2.62233526e-01\n",
      "  5.90281039e-02 -1.41308188e-01 -1.57091394e-01  1.17451862e-01\n",
      " -7.44822063e-03  4.95853834e-02 -9.11567584e-02  1.55987248e-01\n",
      " -2.27995783e-01  2.16111049e-01  1.29257783e-01 -5.49661741e-02\n",
      " -2.01942831e-01 -1.33178145e-01  8.71000439e-02  1.75980702e-01\n",
      "  1.68320999e-01  1.03401160e-03 -5.55450767e-02 -3.18714112e-01\n",
      " -1.21674776e-01  9.95213017e-02 -1.18383765e-01 -1.55251950e-01\n",
      " -8.99694711e-02 -2.04373360e-01 -8.83316174e-02 -4.39998833e-03\n",
      " -8.60141590e-02 -2.06886813e-01 -3.99343789e-01 -4.32852119e-01\n",
      "  2.79969066e-01  2.14414254e-01 -2.59630769e-01 -1.74008831e-01\n",
      " -4.12450172e-02 -4.59151417e-02  1.92073017e-01 -9.76613164e-02\n",
      " -5.95403416e-03 -2.04816684e-01  1.90375194e-01 -2.24221110e-01\n",
      " -1.02648959e-01 -1.10709004e-01  9.74461138e-02 -2.65814543e-01\n",
      "  3.53825814e-03 -5.85060380e-02  1.69196501e-01  1.34627849e-01\n",
      "  2.92710900e-01  8.33644122e-02  8.72158036e-02  9.50062349e-02\n",
      " -4.54250257e-03  3.60987224e-02 -3.26349109e-01 -2.52812773e-01\n",
      " -1.14671320e-01  7.36396834e-02  1.21004105e-01 -1.81255355e-01\n",
      "  1.20265849e-01  1.88138902e-01 -6.31119072e-01  3.39635983e-02\n",
      " -3.53513986e-01  5.98027138e-03  9.78398770e-02 -1.18044205e-01\n",
      " -4.96730983e-01  2.70391911e-01  1.92453489e-01  3.78028035e-01\n",
      " -3.53959203e-01 -4.15919065e-01 -1.20155223e-01 -2.42986590e-01\n",
      "  2.62737460e-03  7.86055773e-02  3.08254033e-01  1.44616261e-01\n",
      " -8.15396942e-03  8.33017752e-02  2.86760658e-01  1.02733962e-01\n",
      "  1.52088195e-01  3.58858287e-01  9.43125188e-02  2.15884540e-02\n",
      "  8.67200792e-02 -5.00753038e-02 -1.86255246e-01 -1.77219212e-01\n",
      " -3.49629894e-02 -4.54627633e-01 -1.95741281e-02 -8.78115594e-02\n",
      "  1.13442272e-01  1.28648309e-02  2.75662899e-01  5.43356650e-02\n",
      "  6.34210780e-02  2.47219786e-01 -2.25749731e-01  2.91284710e-01\n",
      " -1.85625285e-01  9.68681369e-03  1.56890452e-02 -2.41936166e-02\n",
      " -1.05598137e-01 -1.02067359e-01  2.89354682e-01  3.96338284e-01\n",
      " -2.49329805e-01  1.61034867e-01  2.28886291e-01  9.09243226e-02\n",
      " -1.04956791e-01  2.59751454e-02 -9.05998051e-02 -3.41019005e-01\n",
      " -1.66285113e-02 -2.73590416e-01  2.44390652e-01  4.14335400e-01\n",
      " -6.70598820e-02 -4.27496200e-03 -2.08429262e-01 -3.57969314e-01\n",
      " -2.25242320e-03 -9.85382721e-02  1.67275220e-01 -1.57081559e-01\n",
      "  2.81657670e-02  1.37726486e-01  2.38208771e-01 -2.18143035e-02\n",
      " -1.73873648e-01  1.53861463e-01 -9.99423862e-02  4.22778755e-01\n",
      " -3.35183650e-01  1.56653419e-01  1.27213702e-01 -2.76434422e-01\n",
      " -7.71737024e-02 -2.27215573e-01 -1.21324919e-01 -1.20687649e-01\n",
      " -3.04383785e-01  2.11316615e-01 -6.21717311e-02  3.06672156e-01\n",
      "  8.47667903e-02 -1.31748170e-01  4.63135205e-02 -6.77871332e-02\n",
      "  2.31857270e-01  2.70682611e-02  1.55933470e-01  1.65281728e-01\n",
      "  3.54513198e-01  5.52755333e-02 -2.06788070e-02 -1.05801979e-02\n",
      " -1.68989480e-01  2.03013331e-01  1.00364104e-01 -1.69139996e-01\n",
      "  1.32494971e-01 -2.61431545e-01  2.24710152e-01 -7.41541758e-02\n",
      "  2.93617517e-01 -1.01522028e-01 -2.91951764e-02  4.06987555e-02\n",
      "  1.85786337e-01 -1.79797903e-01  9.02459174e-02  7.07829222e-02\n",
      " -3.42507213e-01  3.04847687e-01 -1.91095769e-01 -3.75935793e-01\n",
      "  9.92684886e-02  5.71640916e-02  3.25993486e-02  1.82000846e-01\n",
      "  1.94786787e-01 -9.85409766e-02 -4.98115309e-02 -1.31048813e-01\n",
      "  5.52960634e-01  5.57672158e-02 -1.56942323e-01 -1.14456572e-01\n",
      " -2.20835000e-01 -2.37756997e-01 -2.12476820e-01  5.04613698e-01\n",
      " -2.32271627e-02  3.77694935e-01 -1.07830223e-02  4.88101274e-01\n",
      "  2.14422062e-01  3.25008512e-01  3.94208193e-01 -1.59802675e-01\n",
      "  1.97726995e-01 -1.27075255e-01  2.90122956e-01 -1.80833321e-02\n",
      " -1.11881420e-01 -1.99684411e-01 -2.84844071e-01  2.61450093e-02]\n",
      "Total number of unique words in the vocabulary:\n",
      "134066\n",
      "Length of the vector generated for a word\n",
      "500\n",
      "[ 1.33233115e-01  1.91037953e-01  3.78731340e-02  5.93262464e-02\n",
      " -5.83139211e-02 -3.25685829e-01  7.78143257e-02  3.74059051e-01\n",
      "  1.13592744e-02  7.60043412e-02  7.12478533e-02  1.39684051e-01\n",
      "  5.41280769e-02  1.29098948e-02  1.94703471e-02 -1.61903277e-01\n",
      " -2.00810701e-01 -6.82030842e-02 -2.73511540e-02 -5.99547029e-02\n",
      "  7.39049911e-02 -1.83055505e-01  1.77227408e-01 -2.09274329e-02\n",
      "  1.04506820e-01  1.20798387e-01  1.89920262e-01  4.74798642e-02\n",
      " -2.19326824e-01 -4.77682948e-02  6.38664290e-02 -1.03209071e-01\n",
      " -3.43835913e-02 -6.19486198e-02  1.84101760e-01  8.24857429e-02\n",
      " -3.49336229e-02 -1.79767981e-01 -1.18493371e-01 -1.21554486e-01\n",
      " -1.14339225e-01  3.54865864e-02 -2.74642020e-01  4.53798510e-02\n",
      " -1.57226771e-01 -2.27562934e-01 -1.32526502e-01  1.05985969e-01\n",
      " -1.07581012e-01  7.58007616e-02 -2.55279634e-02 -1.37980089e-01\n",
      " -1.63020808e-02 -1.33144319e-01  1.07353136e-01  2.29351968e-03\n",
      "  3.08811255e-02  8.24764669e-02 -9.41973850e-02  8.65976289e-02\n",
      "  1.30439341e-01 -1.86921060e-02  3.37888151e-02  6.51080208e-03\n",
      " -5.80346659e-02  2.36377984e-01 -9.85560343e-02  2.09219128e-01\n",
      "  1.31026089e-01 -3.34286713e-03 -6.07925281e-02 -3.98186371e-02\n",
      "  1.07603438e-01 -3.57574485e-02  2.42232293e-01  2.16886029e-01\n",
      " -4.40705828e-02 -4.69373725e-02  7.71569312e-02  2.07542032e-01\n",
      " -8.36735666e-02  9.55131203e-02 -1.75139055e-01  2.16147631e-01\n",
      " -1.88360766e-01  1.63063020e-01 -3.30007356e-03  1.04344063e-01\n",
      "  2.04546109e-01  1.84885189e-01  6.09661415e-02  3.29458788e-02\n",
      " -9.01016742e-02  1.46244809e-01  2.04923144e-03  2.76624467e-02\n",
      "  1.09607026e-01 -4.86361906e-02  7.95622692e-02  1.04539089e-01\n",
      " -5.72250523e-02  1.50615185e-01 -2.74359360e-02  1.57411471e-01\n",
      "  8.06977786e-03  7.65729025e-02 -5.59403077e-02  2.60895669e-01\n",
      " -9.20373648e-02  4.13110629e-02 -1.46860434e-02 -1.73030287e-01\n",
      " -2.15657492e-04  2.23088443e-01 -7.60917664e-02  6.49319068e-02\n",
      " -2.15673335e-02 -2.28858724e-01 -4.00024801e-02 -2.24952921e-01\n",
      "  9.01377127e-02 -5.12078740e-02  1.96716681e-01  1.08679682e-02\n",
      " -2.76514255e-02 -2.26365123e-02 -2.13760704e-01  5.87500744e-02\n",
      " -1.17037505e-01  8.72801170e-02  4.68079485e-02  1.41346082e-01\n",
      " -7.71738812e-02 -9.30380728e-03  6.40049651e-02  2.58509696e-01\n",
      " -4.83213319e-03 -4.58061323e-02 -1.37900665e-01 -2.88443059e-01\n",
      "  1.27687886e-01 -1.24479719e-01  8.35403726e-02  1.05778556e-02\n",
      "  1.15464315e-01  4.50282358e-02  1.05229847e-01  1.86853614e-02\n",
      "  1.31155178e-01  2.44254559e-01  1.59595549e-01 -8.09911042e-02\n",
      " -1.05783388e-01  1.59718752e-01 -3.40976745e-01  2.34266017e-02\n",
      "  1.21161185e-01 -1.28306180e-01 -1.19461983e-01 -5.59579954e-02\n",
      "  7.60585591e-02  1.03050575e-01 -2.26623714e-01 -4.28628251e-02\n",
      "  8.57941657e-02  5.94996065e-02  3.50435674e-01  1.67646244e-01\n",
      "  8.95734355e-02  2.80435950e-01  2.00897798e-01  2.08708853e-01\n",
      "  1.97707638e-02  2.82790810e-01  7.08424428e-04 -7.40284175e-02\n",
      " -1.27384111e-01  1.14216395e-02 -2.43039597e-02  1.54530391e-01\n",
      " -1.39625400e-01 -5.91673702e-02 -4.29194942e-02  4.88451570e-02\n",
      " -8.17559287e-02  5.09716794e-02 -2.08978727e-01  9.40732658e-02\n",
      "  1.80448331e-02  1.26173750e-01  1.86679214e-01 -1.48216859e-01\n",
      "  7.39089549e-02  7.32305460e-03  1.03760868e-01 -6.20898046e-02\n",
      " -2.18570642e-02 -1.00762360e-01  1.26122579e-01  3.44688557e-02\n",
      "  8.55302438e-02 -1.84421260e-02  2.29241371e-01 -8.40975530e-03\n",
      "  8.89006183e-02  8.07213262e-02  2.06933841e-02 -8.63671824e-02\n",
      " -1.35474637e-01 -2.22794944e-03 -3.51228304e-02 -2.52635747e-01\n",
      " -1.19708411e-01 -7.27342293e-02  7.62288983e-05 -2.15305582e-01\n",
      " -6.83270022e-02 -8.50114673e-02 -5.69705144e-02 -1.69189200e-01\n",
      "  7.03668445e-02  2.13274777e-01  1.53073013e-01  8.06492865e-02\n",
      "  4.17016000e-02  1.00189038e-02  1.21226497e-01 -2.55887918e-02\n",
      "  1.45142525e-02  4.33134241e-03  1.36281806e-03 -3.35483737e-02\n",
      "  5.37873171e-02  3.55204791e-02 -1.07938208e-01 -1.33921713e-01\n",
      " -1.00965463e-01 -2.05467701e-01  2.00825185e-01 -1.33143649e-01\n",
      "  2.70037558e-02 -1.81119572e-02 -1.93960741e-01  1.77492827e-01\n",
      " -3.56118754e-02 -2.57828832e-01  2.55069863e-02  1.79264992e-02\n",
      "  8.75464380e-02  1.51307791e-01  8.39904174e-02 -1.65531293e-01\n",
      " -7.12829642e-03  3.58401872e-02  4.07876782e-02  6.32970780e-02\n",
      "  1.01211987e-01  2.52866089e-01 -1.93867698e-01  1.09149776e-01\n",
      " -1.06435664e-01 -1.70231145e-02 -1.48843274e-01  6.97838664e-02\n",
      "  1.31831979e-02 -4.35442068e-02 -1.11061424e-01  2.24367484e-01\n",
      " -1.94751203e-01 -3.21753807e-02 -8.70833546e-02  5.12223616e-02\n",
      "  8.35336298e-02 -1.46433860e-01 -5.98128289e-02  3.49622741e-02\n",
      " -9.63023975e-02 -1.77116945e-01 -1.62492543e-01  6.13789596e-02\n",
      "  3.33557650e-02  7.32197613e-02 -1.67381182e-01  9.56481881e-03\n",
      "  2.44432613e-01 -1.31961912e-01 -4.87226844e-02 -4.79588322e-02\n",
      "  2.38425210e-02 -5.39872143e-03 -1.81548987e-02  1.54956236e-01\n",
      "  1.22973667e-02 -1.05721109e-01  1.73298135e-01 -2.50014439e-02\n",
      "  1.36426121e-01 -5.76267168e-02  1.20394863e-01 -9.95198935e-02\n",
      " -1.16089493e-01  7.03790635e-02  5.17643951e-02 -4.36965153e-02\n",
      "  5.56354448e-02 -1.26875803e-01  1.02705453e-02  5.97129986e-02\n",
      "  2.45346464e-02 -1.50281921e-01 -4.00874019e-02 -1.68744102e-01\n",
      "  1.84985530e-02  2.27446452e-01  2.54930696e-03  6.34140521e-02\n",
      " -2.38679219e-02 -1.60798654e-01  3.61172706e-02 -7.45735466e-02\n",
      "  1.06017604e-01 -6.11744486e-02  9.28565785e-02 -8.03266093e-02\n",
      " -5.56949787e-02 -1.25449851e-01 -8.73011500e-02 -8.48845690e-02\n",
      " -3.66275874e-03  5.05527444e-02  6.53571412e-02  1.08244888e-01\n",
      " -5.84650598e-02 -2.54674274e-02  5.16024008e-02 -6.39612377e-02\n",
      " -3.76000144e-02  1.43081531e-01  3.01593766e-02 -6.95659146e-02\n",
      " -9.46004391e-02 -1.06839929e-02  1.59363106e-01 -2.07720041e-01\n",
      "  7.00299367e-02  4.27653641e-02 -2.38824770e-01  3.68447378e-02\n",
      " -2.25946922e-02  4.74306978e-02 -8.05214942e-02 -5.04215322e-02\n",
      " -2.67561972e-01  1.41703904e-01 -3.10837850e-02  6.68623745e-02\n",
      " -2.21171081e-01 -7.12656826e-02 -8.40645507e-02 -2.98481435e-03\n",
      " -1.38045559e-02  4.15890664e-02 -1.32056158e-02  1.30175084e-01\n",
      " -6.09089509e-02 -3.24085951e-02  1.41192824e-01  8.26706178e-03\n",
      " -9.02454779e-02  1.50796711e-01 -4.18658890e-02 -2.53954470e-01\n",
      " -1.14452735e-01 -1.70213617e-02  2.14891098e-02 -3.32021788e-02\n",
      " -1.14131663e-02  6.35396540e-02  2.55189277e-02  1.52690951e-02\n",
      " -1.51411602e-02 -4.27287742e-02  2.41616756e-01  1.28040209e-01\n",
      " -3.75476703e-02  4.37229201e-02  6.34460449e-02  1.17294058e-01\n",
      " -6.01974130e-03 -7.11576417e-02 -5.49563840e-02 -1.42452329e-01\n",
      " -1.99724603e-02  6.55156653e-03  1.28744066e-01  1.82873964e-01\n",
      "  6.33267611e-02 -3.74325551e-02  4.52402271e-02  1.55519158e-01\n",
      " -7.42559880e-02  6.49560168e-02 -1.09296292e-01 -2.03312621e-01\n",
      "  1.80608220e-02 -1.47522464e-01  5.72730526e-02  1.16667539e-01\n",
      "  8.61602500e-02 -9.66336802e-02  2.99694818e-02 -1.63773119e-01\n",
      " -1.31461576e-01 -9.86442417e-02  8.53860229e-02  1.49783954e-01\n",
      "  2.33722463e-01 -1.37008637e-01  1.98242053e-01 -1.72187537e-01\n",
      " -1.16760053e-01  7.22730458e-02 -1.37812078e-01  1.87745169e-01\n",
      " -1.14841945e-01 -2.54449379e-02  4.16221991e-02 -6.82934672e-02\n",
      "  5.29246069e-02 -2.04427183e-01 -3.25892754e-02 -1.12519842e-02\n",
      " -3.44126411e-02  6.61292151e-02 -2.83635035e-02 -1.13204559e-02\n",
      "  6.06632084e-02 -7.97183514e-02 -1.07552014e-01  3.45866531e-02\n",
      "  8.33501443e-02 -3.67005169e-02  2.06746683e-01 -4.27239016e-02\n",
      "  3.08782220e-01  7.85585120e-02  7.14782625e-02  6.26859739e-02\n",
      " -8.98367837e-02 -7.82756135e-02 -3.26221734e-02  2.04810984e-02\n",
      " -3.62985544e-02 -1.64022207e-01  7.15244114e-02 -9.10472125e-02\n",
      "  2.21509160e-03 -8.09747726e-02  1.34601653e-01 -1.41663760e-01\n",
      "  1.39645904e-01  1.19186155e-01  4.78131585e-02  6.93191588e-02\n",
      " -1.05368644e-01 -2.65942197e-02 -3.98352668e-02 -3.37809831e-01\n",
      " -1.44458517e-01  6.45112097e-02 -1.39402092e-01 -1.40340015e-01\n",
      "  1.13968186e-01 -1.56084239e-01  9.36799496e-02  9.07609388e-02\n",
      "  1.53467715e-01 -1.00411087e-01 -1.98554605e-01 -1.55990124e-01\n",
      " -1.58710539e-01 -3.36858928e-01 -1.22278146e-01  2.08773628e-01\n",
      "  1.23061225e-01  2.43001264e-02  2.03975458e-02  1.50252968e-01\n",
      "  1.05790861e-01 -1.13141851e-03  5.50362729e-02 -1.22631676e-01\n",
      "  1.33728594e-01  7.82575160e-02  1.44314274e-01  2.74784900e-02\n",
      " -1.79365219e-03 -1.95576370e-01 -1.81059033e-01  4.18527871e-02]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Load the model from the model file\n",
    "sg_w2v_model = Word2Vec.load(word2vec_model_file)\n",
    "\n",
    "# Unique ID of the word\n",
    "print(\"Index of the word 'action':\")\n",
    "print(sg_w2v_model.wv.get_index(\"action\"))\n",
    "\n",
    "print(\"Vector generated for the word 'action':\")\n",
    "print(sg_w2v_model.wv[\"action\"])\n",
    "\n",
    "# Total Number of the words\n",
    "print(\"Total number of unique words in the vocabulary:\")\n",
    "print(len(sg_w2v_model.wv.index_to_key))\n",
    "\n",
    "# Print the size of the word2vec vector for one word\n",
    "print(\"Length of the vector generated for a word\")\n",
    "print(sg_w2v_model.vector_size)\n",
    "# Get the mean for the vectors for an example review\n",
    "# print(\"Print the length after taking average of all word vectors in a sentence:\")\n",
    "print(np.mean([sg_w2v_model.wv[token] for token in df_train['stemmed_tokens'][0]], axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9822e71",
   "metadata": {},
   "source": [
    "# We now have the Word2Vec model trained on our dataset. We can load this model whenever we want to use it for generating word vectors for our text data.\n",
    "\n",
    "We can make use of our X_train and X_test datasets to create the Word2Vec representations for our training and testing data respectively. We will create two separate CSV files to store these representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21842f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the vectors for train data in following file\n",
    "word2vec_filename = OUTPUT_FOLDER + 'train_review_word2vec.csv'\n",
    "# with open(word2vec_filename, 'w+') as word2vec_file:\n",
    "#     for index, row in X_train.iterrows():\n",
    "#         model_vector = (np.mean([sg_w2v_model.wv[token] for token in row['stemmed_tokens']], axis=0)).tolist()\n",
    "#         if index == 0:\n",
    "#             header = \",\".join(str(ele) for ele in range(size))\n",
    "#             word2vec_file.write(header)\n",
    "#             word2vec_file.write(\"\\n\")\n",
    "#         # Check if the line exists else it is vector of zeros\n",
    "#         if type(model_vector) is list:  \n",
    "#             line1 = \",\".join( [str(vector_element) for vector_element in model_vector] )\n",
    "#         else:\n",
    "#             line1 = \",\".join([str(0) for i in range(size)])\n",
    "#         word2vec_file.write(line1)\n",
    "#         word2vec_file.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae83ba78",
   "metadata": {},
   "source": [
    "## Train Sentiment Classifier via Logistic Regression\n",
    "\n",
    "We'll use scikit-learn's `LogisticRegression` for fast, interpretable binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a3c0fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the word2vec training data: (200630, 500)\n",
      "Shape of Y_train: (200630,)\n",
      "Time taken to train Logistic Regression: 4.107938051223755\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import time\n",
    "\n",
    "# Load the training data from the CSV file\n",
    "word2vec_df = pd.read_csv(OUTPUT_FOLDER + 'train_review_word2vec.csv')\n",
    "\n",
    "print(\"Shape of the word2vec training data:\", word2vec_df.shape)\n",
    "print(\"Shape of Y_train:\", Y_train.shape)\n",
    "\n",
    "# Initialize Logistic Regression with parallel jobs\n",
    "logreg = LogisticRegression(max_iter=200, n_jobs=-1, random_state=42, class_weight=\"balanced\")\n",
    "\n",
    "start_time = time.time()\n",
    "logreg.fit(word2vec_df, Y_train)\n",
    "print(\"Time taken to train Logistic Regression: \" + str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74ded173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.63      0.77     43031\n",
      "           1       0.23      0.96      0.37      4841\n",
      "\n",
      "    accuracy                           0.67     47872\n",
      "   macro avg       0.61      0.80      0.57     47872\n",
      "weighted avg       0.92      0.67      0.73     47872\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Logistic Regression on test set\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "test_features_word2vec = []\n",
    "vec_size = sg_w2v_model.vector_size  # should be 500\n",
    "\n",
    "for _, row in X_test.iterrows():\n",
    "    vectors = [sg_w2v_model.wv[t] for t in row[\"stemmed_tokens\"] if t in\n",
    "sg_w2v_model.wv]\n",
    "    test_features_word2vec.append(np.mean(vectors, axis=0) if vectors else\n",
    "np.zeros(vec_size))\n",
    "\n",
    "test_features_word2vec = pd.DataFrame(test_features_word2vec,\n",
    "columns=word2vec_df.columns)\n",
    "test_predictions_logreg = logreg.predict(test_features_word2vec)\n",
    "print(classification_report(Y_test, test_predictions_logreg, zero_division=0))\n",
    "# # Evaluate Logistic Regression on test set\n",
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "# test_features_word2vec = []\n",
    "# for index, row in X_test.iterrows():\n",
    "#     model_vector = np.mean([sg_w2v_model.wv[token] for token in row['stemmed_tokens']], axis=0)\n",
    "#     if type(model_vector) is list:\n",
    "#         test_features_word2vec.append(model_vector)\n",
    "#     else:\n",
    "#         test_features_word2vec.append(np.array([0 for i in range(500)]))\n",
    "\n",
    "# test_features_word2vec = pd.DataFrame(test_features_word2vec, columns=word2vec_df.columns)\n",
    "# test_predictions_logreg = logreg.predict(test_features_word2vec)\n",
    "# print(classification_report(Y_test, test_predictions_logreg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c744720a",
   "metadata": {},
   "source": [
    "# Train Sentiment Classifier via Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2d0774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to train Random Forest Classifier: 36.495548486709595\n"
     ]
    }
   ],
   "source": [
    "# import time\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced', n_jobs=-1)\n",
    "# # Load the training data from the CSV file\n",
    "# word2vec_df = pd.read_csv(OUTPUT_FOLDER + 'train_review_word2vec.csv')\n",
    "\n",
    "# start_time = time.time()\n",
    "# # Fit the Random Forest Classifier\n",
    "# rf_classifier.fit(word2vec_df, Y_train)\n",
    "# print(\"Time taken to train Random Forest Classifier: \" + str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2496f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ryam/Code/SafuForum/ai/.venv/lib/python3.13/site-packages/numpy/_core/fromnumeric.py:3860: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/ryam/Code/SafuForum/ai/.venv/lib/python3.13/site-packages/numpy/_core/_methods.py:144: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00     43031\n",
      "           1       0.10      1.00      0.18      4841\n",
      "\n",
      "    accuracy                           0.10     47872\n",
      "   macro avg       0.05      0.50      0.09     47872\n",
      "weighted avg       0.01      0.10      0.02     47872\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ryam/Code/SafuForum/ai/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/ryam/Code/SafuForum/ai/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/ryam/Code/SafuForum/ai/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/ryam/Code/SafuForum/ai/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.metrics import classification_report\n",
    "# test_features_word2vec = []\n",
    "# for index, row in X_test.iterrows():\n",
    "#     model_vector = np.mean([sg_w2v_model.wv[token] for token in row['stemmed_tokens']], axis=0)\n",
    "#     if type(model_vector) is list:\n",
    "#         test_features_word2vec.append(model_vector)\n",
    "#     else:\n",
    "#         test_features_word2vec.append(np.array([0 for i in range(size)]))\n",
    "# test_predictions_word2vec = rf_classifier.predict(test_features_word2vec)\n",
    "# print(classification_report(Y_test,test_predictions_word2vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308dbac5",
   "metadata": {},
   "source": [
    "# Try with Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bd98e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# #Import the DecisionTreeeClassifier\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# # Load from the filename\n",
    "# word2vec_df = pd.read_csv(word2vec_filename)\n",
    "# #Initialize the model\n",
    "# clf_decision_word2vec = DecisionTreeClassifier()\n",
    "\n",
    "# start_time = time.time()\n",
    "# # Fit the model\n",
    "# clf_decision_word2vec.fit(word2vec_df, Y_train)\n",
    "# print(\"Time taken to fit the model with word2vec vectors: \" + str(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd08363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import classification_report\n",
    "# test_features_word2vec = []\n",
    "# for index, row in X_test.iterrows():\n",
    "#     model_vector = np.mean([sg_w2v_model[token] for token in row['stemmed_tokens']], axis=0)\n",
    "#     if type(model_vector) is list:\n",
    "#         test_features_word2vec.append(model_vector)\n",
    "#     else:\n",
    "#         test_features_word2vec.append(np.array([0 for i in range(500)]))\n",
    "# test_predictions_word2vec = clf_decision_word2vec.predict(test_features_word2vec)\n",
    "# print(classification_report(Y_test['sentiment'],test_predictions_word2vec))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
